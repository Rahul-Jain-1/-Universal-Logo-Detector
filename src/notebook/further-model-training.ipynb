{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T16:52:32.143323Z",
     "iopub.status.busy": "2025-05-27T16:52:32.142554Z",
     "iopub.status.idle": "2025-05-27T16:52:59.477850Z",
     "shell.execute_reply": "2025-05-27T16:52:59.477093Z",
     "shell.execute_reply.started": "2025-05-27T16:52:32.143296Z"
    },
    "id": "MpDzhLyjo6mP",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-27 16:52:45.590832: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748364765.821328      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748364765.889183      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "\n",
    "import json\n",
    "import yaml\n",
    "\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "from lxml import etree as ET\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoImageProcessor, AutoModelForObjectDetection\n",
    "import torch\n",
    "from PIL import Image, ImageDraw, ImageFont\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T16:52:59.479766Z",
     "iopub.status.busy": "2025-05-27T16:52:59.479286Z",
     "iopub.status.idle": "2025-05-27T16:52:59.483696Z",
     "shell.execute_reply": "2025-05-27T16:52:59.482990Z",
     "shell.execute_reply.started": "2025-05-27T16:52:59.479747Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import logging as hf_logging\n",
    "\n",
    "# suppress everything below ERROR\n",
    "hf_logging.set_verbosity_error()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-05-27T16:52:59.484700Z",
     "iopub.status.busy": "2025-05-27T16:52:59.484446Z",
     "iopub.status.idle": "2025-05-27T16:52:59.502229Z",
     "shell.execute_reply": "2025-05-27T16:52:59.501615Z",
     "shell.execute_reply.started": "2025-05-27T16:52:59.484678Z"
    },
    "id": "wWx2i8swpMYo",
    "outputId": "3d9e8b59-2d88-42e4-dca0-89b2f2232969",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "classname2idx = {\"logo\": 0}\n",
    "idx2classname =  {0:\"logo\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T16:52:59.503999Z",
     "iopub.status.busy": "2025-05-27T16:52:59.503759Z",
     "iopub.status.idle": "2025-05-27T16:52:59.518715Z",
     "shell.execute_reply": "2025-05-27T16:52:59.518205Z",
     "shell.execute_reply.started": "2025-05-27T16:52:59.503980Z"
    },
    "id": "o2YlCAU-q7K_",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "class LogoDataset(Dataset):\n",
    "    def __init__(self, dataset, image_processor, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.image_processor = image_processor\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.dataset[idx]\n",
    "\n",
    "        image_path = sample[\"file_path\"]\n",
    "        image = Image.open(image_path)\n",
    "        formatted_annotations = sample[\"annotation\"]\n",
    "\n",
    "        # Convert image to RGB numpy array\n",
    "        image = np.array(image.convert(\"RGB\"))\n",
    "\n",
    "        result = self.image_processor(\n",
    "            images=image, annotations=formatted_annotations, return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Image processor expands batch dimension, lets squeeze it\n",
    "        result = {k: v[0] for k, v in result.items()}\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T16:52:59.519761Z",
     "iopub.status.busy": "2025-05-27T16:52:59.519528Z",
     "iopub.status.idle": "2025-05-27T16:53:02.254257Z",
     "shell.execute_reply": "2025-05-27T16:53:02.253509Z",
     "shell.execute_reply.started": "2025-05-27T16:52:59.519745Z"
    },
    "id": "ixGE57L-pPuz",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/albumentations/__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.7' (you have '2.0.5'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "train_path = \"/kaggle/input/dataset-logo/train_dataset.pkl\"\n",
    "valid_path = \"/kaggle/input/dataset-logo/validation_dataset.pkl\"\n",
    "\n",
    "with open(train_path, \"rb\") as f:\n",
    "    train = pickle.load(f)\n",
    "\n",
    "with open(valid_path, \"rb\") as f:\n",
    "    valid = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T16:53:02.255340Z",
     "iopub.status.busy": "2025-05-27T16:53:02.255046Z",
     "iopub.status.idle": "2025-05-27T16:53:02.359237Z",
     "shell.execute_reply": "2025-05-27T16:53:02.358651Z",
     "shell.execute_reply.started": "2025-05-27T16:53:02.255315Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pixel_values': tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       " \n",
       "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       " \n",
       "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.]]]),\n",
       " 'labels': {'size': tensor([480, 480]), 'image_id': tensor([0]), 'class_labels': tensor([0]), 'boxes': tensor([[0.5041, 0.5519, 0.4888, 0.2154]]), 'area': tensor([24258.7656]), 'iscrowd': tensor([0]), 'orig_size': tensor([376, 489])}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train model on (5) % of data ... this time we train on 2nd subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T16:54:03.652247Z",
     "iopub.status.busy": "2025-05-27T16:54:03.651879Z",
     "iopub.status.idle": "2025-05-27T16:54:03.683065Z",
     "shell.execute_reply": "2025-05-27T16:54:03.682343Z",
     "shell.execute_reply.started": "2025-05-27T16:54:03.652225Z"
    },
    "id": "pcsVe6uLpgb9",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import torch\n",
    "\n",
    "# 1) Clear any stray allocations\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"rtdetr-v2-finetune-on-10-percent\",\n",
    "    num_train_epochs=5,\n",
    "    max_grad_norm=0.1,\n",
    "    learning_rate=5e-5,\n",
    "    warmup_steps=300,\n",
    "    per_device_train_batch_size=8,\n",
    "    dataloader_num_workers=2,\n",
    "    metric_for_best_model=\"eval_map\",\n",
    "    greater_is_better=True,\n",
    "    load_best_model_at_end=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=True,\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    "    eval_do_concat_batches=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T16:53:02.360659Z",
     "iopub.status.busy": "2025-05-27T16:53:02.360373Z",
     "iopub.status.idle": "2025-05-27T16:53:02.364845Z",
     "shell.execute_reply": "2025-05-27T16:53:02.364087Z",
     "shell.execute_reply.started": "2025-05-27T16:53:02.360631Z"
    },
    "id": "bcyAgL8arMHy",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def collate_fn(batch):\n",
    "    data = {}\n",
    "    data[\"pixel_values\"] = torch.stack([x[\"pixel_values\"] for x in batch])\n",
    "    data[\"labels\"] = [x[\"labels\"] for x in batch]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load finetune model that trained on first 5% data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T16:53:27.757374Z",
     "iopub.status.busy": "2025-05-27T16:53:27.756670Z",
     "iopub.status.idle": "2025-05-27T16:53:27.768924Z",
     "shell.execute_reply": "2025-05-27T16:53:27.768199Z",
     "shell.execute_reply.started": "2025-05-27T16:53:27.757350Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import AutoImageProcessor\n",
    "checkpoint = \"/kaggle/input/rtdter/pytorch/default/1\"\n",
    "image_size = 480\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\n",
    "    checkpoint,\n",
    "    do_resize=True,\n",
    "    size={\"width\": image_size, \"height\": image_size},\n",
    "    use_fast=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T16:53:31.249373Z",
     "iopub.status.busy": "2025-05-27T16:53:31.249121Z",
     "iopub.status.idle": "2025-05-27T16:53:32.061220Z",
     "shell.execute_reply": "2025-05-27T16:53:32.060656Z",
     "shell.execute_reply.started": "2025-05-27T16:53:31.249357Z"
    },
    "id": "c_ge_vXNrUKI",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from transformers.image_transforms import center_to_corners_format\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelOutput:\n",
    "    logits: torch.Tensor\n",
    "    pred_boxes: torch.Tensor\n",
    "\n",
    "\n",
    "class MAPEvaluator:\n",
    "\n",
    "    def __init__(self, image_processor, threshold=0.00, id2label=None):\n",
    "        self.image_processor = image_processor\n",
    "        self.threshold = threshold\n",
    "        self.id2label = id2label\n",
    "\n",
    "    def collect_image_sizes(self, targets):\n",
    "        \"\"\"Collect image sizes across the dataset as list of tensors with shape [batch_size, 2].\"\"\"\n",
    "        image_sizes = []\n",
    "        for batch in targets:\n",
    "            batch_image_sizes = torch.tensor(np.array([x[\"size\"] for x in batch]))\n",
    "            image_sizes.append(batch_image_sizes)\n",
    "        return image_sizes\n",
    "\n",
    "    def collect_targets(self, targets, image_sizes):\n",
    "        post_processed_targets = []\n",
    "        for target_batch, image_size_batch in zip(targets, image_sizes):\n",
    "            for target, size in zip(target_batch, image_size_batch):\n",
    "\n",
    "                # here we have \"yolo\" format (x_center, y_center, width, height) in relative coordinates 0..1\n",
    "                # and we need to convert it to \"pascal\" format (x_min, y_min, x_max, y_max) in absolute coordinates\n",
    "                height, width = size\n",
    "                boxes = torch.tensor(target[\"boxes\"])\n",
    "                boxes = center_to_corners_format(boxes)\n",
    "                boxes = boxes * torch.tensor([[width, height, width, height]])\n",
    "\n",
    "                labels = torch.tensor(target[\"class_labels\"])\n",
    "                post_processed_targets.append({\"boxes\": boxes, \"labels\": labels})\n",
    "        return post_processed_targets\n",
    "\n",
    "    def collect_predictions(self, predictions, image_sizes):\n",
    "        post_processed_predictions = []\n",
    "        for batch, target_sizes in zip(predictions, image_sizes):\n",
    "            batch_logits, batch_boxes = batch[1], batch[2]\n",
    "            output = ModelOutput(logits=torch.tensor(batch_logits), pred_boxes=torch.tensor(batch_boxes))\n",
    "            post_processed_output = self.image_processor.post_process_object_detection(\n",
    "                output, threshold=self.threshold, target_sizes=target_sizes\n",
    "            )\n",
    "            post_processed_predictions.extend(post_processed_output)\n",
    "        return post_processed_predictions\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(self, evaluation_results):\n",
    "        # 1) Pre- and post-process your preds & targets\n",
    "        predictions, targets = evaluation_results.predictions, evaluation_results.label_ids\n",
    "\n",
    "        image_sizes = self.collect_image_sizes(targets)\n",
    "        post_processed_targets = self.collect_targets(targets, image_sizes)\n",
    "        post_processed_predictions = self.collect_predictions(predictions, image_sizes)\n",
    "    \n",
    "        # 2) Instantiate the metric under the name \"evaluator\"\n",
    "        evaluator = MeanAveragePrecision(box_format=\"xyxy\", class_metrics=True)\n",
    "        evaluator.warn_on_many_detections = False\n",
    "        evaluator.update(post_processed_predictions, post_processed_targets)\n",
    "\n",
    "        metrics = evaluator.compute()\n",
    "    \n",
    "        # …then your wrapping of per-class metrics, rounding, etc…\n",
    "        return metrics\n",
    "\n",
    "\n",
    "eval_compute_metrics_fn = MAPEvaluator(image_processor=image_processor, threshold=0.01, id2label=idx2classname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T16:53:34.488376Z",
     "iopub.status.busy": "2025-05-27T16:53:34.487549Z",
     "iopub.status.idle": "2025-05-27T16:53:34.491896Z",
     "shell.execute_reply": "2025-05-27T16:53:34.490984Z",
     "shell.execute_reply.started": "2025-05-27T16:53:34.488352Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_API_KEY\"] = os.getenv(\"WANDB_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T17:29:50.383591Z",
     "iopub.status.busy": "2025-05-27T17:29:50.382995Z",
     "iopub.status.idle": "2025-05-27T18:22:55.426052Z",
     "shell.execute_reply": "2025-05-27T18:22:55.425222Z",
     "shell.execute_reply.started": "2025-05-27T17:29:50.383569Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skip 1st subset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35/961709877.py:61: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training subset 2/10 (train idx 6346:12692, valid idx 1586:3172) ===\n",
      "{'loss': 14.2229, 'grad_norm': 27.506196975708008, 'learning_rate': 4.380352644836273e-05, 'epoch': 0.6297229219143576}\n",
      "{'loss': 13.8249, 'grad_norm': 42.66318893432617, 'learning_rate': 3.7506297229219146e-05, 'epoch': 1.2594458438287153}\n",
      "{'loss': 13.1307, 'grad_norm': 45.53601837158203, 'learning_rate': 3.1221662468513854e-05, 'epoch': 1.8891687657430731}\n",
      "{'loss': 12.6174, 'grad_norm': 24.074769973754883, 'learning_rate': 2.4924433249370276e-05, 'epoch': 2.5188916876574305}\n",
      "{'loss': 12.2288, 'grad_norm': 36.361873626708984, 'learning_rate': 1.86272040302267e-05, 'epoch': 3.1486146095717884}\n",
      "{'loss': 11.8171, 'grad_norm': 25.454195022583008, 'learning_rate': 1.2329974811083123e-05, 'epoch': 3.7783375314861463}\n",
      "{'loss': 11.4877, 'grad_norm': 24.974241256713867, 'learning_rate': 6.045340050377834e-06, 'epoch': 4.408060453400504}\n",
      "{'train_runtime': 2556.622, 'train_samples_per_second': 12.411, 'train_steps_per_second': 1.553, 'train_loss': 12.577334156564861, 'epoch': 5.0}\n",
      "Saved & zipped to .//model_subset_2.zip\n",
      "Cleared CUDA cache.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35/961709877.py:61: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training subset 3/10 (train idx 12692:19038, valid idx 3172:4758) ===\n",
      "{'loss': 14.1414, 'grad_norm': 50.41816711425781, 'learning_rate': 4.380352644836273e-05, 'epoch': 0.6297229219143576}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_35/961709877.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m           f\"valid idx {start_val}:{end_val}) ===\")\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;31m# --- save & zip ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2243\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2244\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2245\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2246\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2247\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2563\u001b[0m                         \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging_nan_inf_filter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2564\u001b[0m                         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_torch_xla_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2565\u001b[0;31m                         \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss_step\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misinf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2566\u001b[0m                     ):\n\u001b[1;32m   2567\u001b[0m                         \u001b[0;31m# if loss is nan or inf simply add the average of previous logged losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import torch\n",
    "from torch.utils.data import Subset\n",
    "from transformers import (\n",
    "    AutoModelForImageClassification,     # or your specific class\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "\n",
    "# --- CONFIG ---\n",
    "FINETUNED_CHECKPOINT = \"/kaggle/input/rtdter/pytorch/default/1\"\n",
    "OUTPUT_BASE_DIR      = \"./\"\n",
    "SUBSET_FRAC          = 0.05       # 5%\n",
    "MAX_COVER_FRAC       = 0.50       # up to 50% total\n",
    "SEED                 = 42\n",
    "\n",
    "# --- Datasets (your existing train/valid) ---\n",
    "# train = ...\n",
    "# valid = ...\n",
    "\n",
    "# load model\n",
    "\n",
    "# compute sizes\n",
    "train_size = len(train)\n",
    "valid_size = len(valid)\n",
    "subset_train_size = int(SUBSET_FRAC * train_size)\n",
    "subset_valid_size = int(SUBSET_FRAC * valid_size)\n",
    "num_subsets = int(MAX_COVER_FRAC / SUBSET_FRAC)  # == 10 for 50%\n",
    "\n",
    "for i in range(num_subsets):\n",
    "    if i==0:\n",
    "        print(\"skip 1st subset\")\n",
    "        continue\n",
    "    start_train = i * subset_train_size\n",
    "    end_train   = start_train + subset_train_size\n",
    "    start_val   = i * subset_valid_size\n",
    "    end_val     = start_val + subset_valid_size\n",
    "\n",
    "    # make Subsets\n",
    "    train_subset = Subset(train, list(range(start_train, end_train)))\n",
    "    valid_subset = Subset(valid, list(range(start_val, end_val)))\n",
    "\n",
    "    # reset model to the fine‐tuned state for each chunk\n",
    "    model = AutoModelForObjectDetection.from_pretrained(FINETUNED_CHECKPOINT)\n",
    "    model.to(\"cuda\")\n",
    "\n",
    "    # setup training args\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"{OUTPUT_BASE_DIR}/model_subset_{i+1}\",\n",
    "        per_device_train_batch_size=training_args.per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=training_args.per_device_eval_batch_size,\n",
    "        num_train_epochs=training_args.num_train_epochs,\n",
    "        learning_rate=training_args.learning_rate,\n",
    "        logging_steps=training_args.logging_steps,\n",
    "        save_strategy=\"no\",            # we'll save manually\n",
    "        eval_strategy=\"no\",\n",
    "        seed=SEED,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_subset,\n",
    "        eval_dataset=valid_subset,\n",
    "        tokenizer=image_processor,\n",
    "        data_collator=collate_fn,\n",
    "        compute_metrics=eval_compute_metrics_fn,\n",
    "    )\n",
    "\n",
    "    print(f\"\\n=== Training subset {i+1}/{num_subsets} \"\n",
    "          f\"(train idx {start_train}:{end_train}, \"\n",
    "          f\"valid idx {start_val}:{end_val}) ===\")\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # --- save & zip ---\n",
    "    subset_dir = training_args.output_dir\n",
    "    trainer.save_model(subset_dir)\n",
    "\n",
    "    zip_path = f\"{subset_dir}.zip\"\n",
    "    with zipfile.ZipFile(zip_path, \"w\", zipfile.ZIP_DEFLATED) as zf:\n",
    "        for root, _, files in os.walk(subset_dir):\n",
    "            for fn in files:\n",
    "                fullpath = os.path.join(root, fn)\n",
    "                arcname  = os.path.relpath(fullpath, subset_dir)\n",
    "                zf.write(fullpath, arcname)\n",
    "\n",
    "    print(f\"Saved & zipped to {zip_path}\")\n",
    "\n",
    "    # clear CUDA\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Cleared CUDA cache.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have finetuned model on 2nd subset of 5% of data and save finetuned model (that is trained on 10% of data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we trained a model on finetuned_on_10_percent_data model.. this time we dont evualte model just train it on 25% of data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T18:55:41.761373Z",
     "iopub.status.busy": "2025-05-27T18:55:41.760714Z",
     "iopub.status.idle": "2025-05-27T22:34:26.779816Z",
     "shell.execute_reply": "2025-05-27T22:34:26.779040Z",
     "shell.execute_reply.started": "2025-05-27T18:55:41.761352Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35/1912212904.py:81: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 13.3178, 'grad_norm': 23.39399528503418, 'learning_rate': 4.876228888328712e-05, 'epoch': 0.12603982858583312}\n",
      "{'loss': 13.5126, 'grad_norm': 98.26414489746094, 'learning_rate': 4.750189059742879e-05, 'epoch': 0.25207965717166625}\n",
      "{'loss': 13.2577, 'grad_norm': 26.730506896972656, 'learning_rate': 4.6241492311570464e-05, 'epoch': 0.3781194857574994}\n",
      "{'loss': 13.0447, 'grad_norm': 38.708953857421875, 'learning_rate': 4.498109402571213e-05, 'epoch': 0.5041593143433325}\n",
      "{'loss': 13.0322, 'grad_norm': 386.195068359375, 'learning_rate': 4.372321653642552e-05, 'epoch': 0.6301991429291656}\n",
      "{'loss': 12.7154, 'grad_norm': 50.50520706176758, 'learning_rate': 4.2462818250567184e-05, 'epoch': 0.7562389715149987}\n",
      "{'loss': 12.71, 'grad_norm': 35.673072814941406, 'learning_rate': 4.120241996470885e-05, 'epoch': 0.8822788001008318}\n",
      "{'loss': 12.5111, 'grad_norm': 29.194944381713867, 'learning_rate': 3.994202167885052e-05, 'epoch': 1.008318628686665}\n",
      "{'loss': 12.1101, 'grad_norm': 274.85400390625, 'learning_rate': 3.8681623392992187e-05, 'epoch': 1.134358457272498}\n",
      "{'loss': 12.0196, 'grad_norm': 31.009206771850586, 'learning_rate': 3.742374590370557e-05, 'epoch': 1.2603982858583311}\n",
      "{'loss': 12.0857, 'grad_norm': 25.33870506286621, 'learning_rate': 3.616334761784724e-05, 'epoch': 1.3864381144441644}\n",
      "{'loss': 11.907, 'grad_norm': 31.249454498291016, 'learning_rate': 3.490294933198891e-05, 'epoch': 1.5124779430299975}\n",
      "{'loss': 11.7511, 'grad_norm': 30.660335540771484, 'learning_rate': 3.3642551046130574e-05, 'epoch': 1.6385177716158306}\n",
      "{'loss': 11.733, 'grad_norm': 32.25067901611328, 'learning_rate': 3.238215276027225e-05, 'epoch': 1.7645576002016639}\n",
      "{'loss': 11.7997, 'grad_norm': 31.823354721069336, 'learning_rate': 3.1121754474413916e-05, 'epoch': 1.8905974287874967}\n",
      "{'loss': 11.6152, 'grad_norm': 27.903980255126953, 'learning_rate': 2.9861356188555583e-05, 'epoch': 2.01663725737333}\n",
      "{'loss': 11.3368, 'grad_norm': 26.050678253173828, 'learning_rate': 2.860347869926897e-05, 'epoch': 2.1426770859591633}\n",
      "{'loss': 11.266, 'grad_norm': 28.27168846130371, 'learning_rate': 2.7343080413410636e-05, 'epoch': 2.268716914544996}\n",
      "{'loss': 11.1434, 'grad_norm': 42.882503509521484, 'learning_rate': 2.608268212755231e-05, 'epoch': 2.3947567431308294}\n",
      "{'loss': 11.1085, 'grad_norm': 28.980783462524414, 'learning_rate': 2.4822283841693978e-05, 'epoch': 2.5207965717166623}\n",
      "{'loss': 11.1505, 'grad_norm': 36.441646575927734, 'learning_rate': 2.3561885555835645e-05, 'epoch': 2.6468364003024956}\n",
      "{'loss': 11.018, 'grad_norm': 41.45378494262695, 'learning_rate': 2.230400806654903e-05, 'epoch': 2.772876228888329}\n",
      "{'loss': 10.9461, 'grad_norm': 306.6121520996094, 'learning_rate': 2.10436097806907e-05, 'epoch': 2.8989160574741617}\n",
      "{'loss': 10.9319, 'grad_norm': 100.54603576660156, 'learning_rate': 1.978321149483237e-05, 'epoch': 3.024955886059995}\n",
      "{'loss': 10.6209, 'grad_norm': 26.157180786132812, 'learning_rate': 1.8522813208974037e-05, 'epoch': 3.1509957146458283}\n",
      "{'loss': 10.5702, 'grad_norm': 38.051204681396484, 'learning_rate': 1.7262414923115704e-05, 'epoch': 3.277035543231661}\n",
      "{'loss': 10.5005, 'grad_norm': 459.86907958984375, 'learning_rate': 1.6004537433829093e-05, 'epoch': 3.4030753718174944}\n",
      "{'loss': 10.5301, 'grad_norm': 27.805442810058594, 'learning_rate': 1.4744139147970759e-05, 'epoch': 3.5291152004033277}\n",
      "{'loss': 10.4253, 'grad_norm': 21.698562622070312, 'learning_rate': 1.348374086211243e-05, 'epoch': 3.6551550289891606}\n",
      "{'loss': 10.425, 'grad_norm': 24.569271087646484, 'learning_rate': 1.2223342576254097e-05, 'epoch': 3.781194857574994}\n",
      "{'loss': 10.3909, 'grad_norm': 22.87534523010254, 'learning_rate': 1.0962944290395766e-05, 'epoch': 3.9072346861608267}\n",
      "{'loss': 10.3107, 'grad_norm': 28.28692054748535, 'learning_rate': 9.705066801109152e-06, 'epoch': 4.03327451474666}\n",
      "{'loss': 10.1519, 'grad_norm': 19.81009864807129, 'learning_rate': 8.44466851525082e-06, 'epoch': 4.159314343332493}\n",
      "{'loss': 10.0664, 'grad_norm': 25.77894401550293, 'learning_rate': 7.186791025964205e-06, 'epoch': 4.285354171918327}\n",
      "{'loss': 9.9957, 'grad_norm': 29.381385803222656, 'learning_rate': 5.926392740105874e-06, 'epoch': 4.411394000504159}\n",
      "{'loss': 10.0785, 'grad_norm': 77.74872589111328, 'learning_rate': 4.665994454247542e-06, 'epoch': 4.537433829089992}\n",
      "{'loss': 9.9021, 'grad_norm': 62.12006378173828, 'learning_rate': 3.4055961683892113e-06, 'epoch': 4.663473657675826}\n",
      "{'loss': 9.8453, 'grad_norm': 27.191137313842773, 'learning_rate': 2.1477186791025967e-06, 'epoch': 4.789513486261659}\n",
      "{'loss': 9.9838, 'grad_norm': 21.669998168945312, 'learning_rate': 8.873203932442653e-07, 'epoch': 4.915553314847492}\n",
      "{'train_runtime': 13114.1129, 'train_samples_per_second': 12.098, 'train_steps_per_second': 1.512, 'train_loss': 11.303982932434774, 'epoch': 5.0}\n",
      "Saved & zipped to .//ON-WHOLE-dataset.zip\n",
      "Cleared CUDA cache.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import torch\n",
    "from torch.utils.data import Subset\n",
    "from transformers import (\n",
    "    AutoModelForImageClassification,     # or your specific class\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "\n",
    "# --- CONFIG ---\n",
    "# 1. compute split sizes\n",
    "total = len(train)\n",
    "n_small = int(0.25 * total)\n",
    "n_rest  = total - n_small\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "small_train_ds, _ = random_split(\n",
    "    train,\n",
    "    [n_small, n_rest],\n",
    "    generator=generator\n",
    ")\n",
    "\n",
    "OUTPUT_BASE_DIR      = \"./\"\n",
    "SUBSET_FRAC          = 0.1       # 10%\n",
    "MAX_COVER_FRAC       = 0.50       # up to 50% total\n",
    "SEED                 = 42\n",
    "\n",
    "# --- Datasets (your existing train/valid) ---\n",
    "# train = ...\n",
    "# valid = ...\n",
    "\n",
    "# taking first 5% records only  from train dataset\n",
    "import torch\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "# 1. compute split sizes\n",
    "total = len(train)\n",
    "n_small = int(0.25 * total)\n",
    "\n",
    "# 2. do the random split (with a fixed seed for reproducibility)\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "small_train_ds, set_25,_,_ = random_split(\n",
    "    train,\n",
    "    [n_small, n_small, n_small+2, n_small+1],\n",
    "    generator=generator\n",
    ")\n",
    "\n",
    "total_valid = len(valid)\n",
    "n_small_valid = int(0.25 * total_valid)\n",
    "\n",
    "# 2. do the random split (with a fixed seed for reproducibility)\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "small_valid__ds, valid_set_25,_,_ = random_split(\n",
    "    valid,\n",
    "    [n_small_valid, n_small_valid, n_small_valid+2, n_small_valid+1],\n",
    "    generator=generator\n",
    ")\n",
    "# load model\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# reset model to the fine‐tuned state for each chunk\n",
    "FINETUNED_CHECKPOINT = \"/kaggle/working/model_subset_2\"\n",
    "model = AutoModelForObjectDetection.from_pretrained(FINETUNED_CHECKPOINT)\n",
    "model.to(\"cuda\")\n",
    "\n",
    "    # setup training args\n",
    "NEW_FINETUNED_CHECKPOINT = f\"{OUTPUT_BASE_DIR}/ON-WHOLE-dataset\"\n",
    "training_args = TrainingArguments(\n",
    "        output_dir=NEW_FINETUNED_CHECKPOINT,\n",
    "        per_device_train_batch_size=training_args.per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=training_args.per_device_eval_batch_size,\n",
    "        num_train_epochs=training_args.num_train_epochs,\n",
    "        learning_rate=training_args.learning_rate,\n",
    "        logging_steps=training_args.logging_steps,\n",
    "        save_strategy=\"no\",            # we'll save manually\n",
    "        eval_strategy=\"no\",\n",
    "        seed=SEED,\n",
    "    )\n",
    "\n",
    "trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=set_25,\n",
    "        eval_dataset=valid_set_25,\n",
    "        tokenizer=image_processor,\n",
    "        data_collator=collate_fn,\n",
    "        compute_metrics=eval_compute_metrics_fn,\n",
    "    )\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "    # --- save & zip ---\n",
    "subset_dir = training_args.output_dir\n",
    "trainer.save_model(subset_dir)\n",
    "\n",
    "zip_path = f\"{subset_dir}.zip\"\n",
    "with zipfile.ZipFile(zip_path, \"w\", zipfile.ZIP_DEFLATED) as zf:\n",
    "   for root, _, files in os.walk(subset_dir):\n",
    "        for fn in files:\n",
    "              fullpath = os.path.join(root, fn)\n",
    "              arcname  = os.path.relpath(fullpath, subset_dir)\n",
    "              zf.write(fullpath, arcname)\n",
    "\n",
    "   print(f\"Saved & zipped to {zip_path}\")\n",
    "    \n",
    "    # clear CUDA\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Cleared CUDA cache.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "saved the final finetuned model that is trained on 35 % of whole dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 888440,
     "sourceId": 1508223,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7529830,
     "sourceId": 11973838,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7526769,
     "sourceId": 11969551,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 359282,
     "modelInstanceId": 338336,
     "sourceId": 414489,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "19db15cbe36e4f6983d52c572e94b29c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_afe4e6215fa541218db50b6a8c1aca82",
       "IPY_MODEL_2eda47164bcd4aaaaec3f5ada927be06",
       "IPY_MODEL_7f100bf8cbc14635b0960e08dbee111c"
      ],
      "layout": "IPY_MODEL_f6b4b9314feb4fb18861d60e037f9edf"
     }
    },
    "2eda47164bcd4aaaaec3f5ada927be06": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3b23ab87600b45c2a6cab506dbeca5bf",
      "max": 444,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5a03aa472df246469d8327ca7e4e7c0f",
      "value": 444
     }
    },
    "3b23ab87600b45c2a6cab506dbeca5bf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5a03aa472df246469d8327ca7e4e7c0f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5dd99c76deb048be99e972d8552e8f54": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7f100bf8cbc14635b0960e08dbee111c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9d901617998043e0a96f382d28268133",
      "placeholder": "​",
      "style": "IPY_MODEL_5dd99c76deb048be99e972d8552e8f54",
      "value": " 444/444 [00:00&lt;00:00, 47.8kB/s]"
     }
    },
    "8e980a665aeb46b0a643fc618c7e4781": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "985c201e73a94890a8dae7650937acc5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9d901617998043e0a96f382d28268133": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "afe4e6215fa541218db50b6a8c1aca82": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_985c201e73a94890a8dae7650937acc5",
      "placeholder": "​",
      "style": "IPY_MODEL_8e980a665aeb46b0a643fc618c7e4781",
      "value": "preprocessor_config.json: 100%"
     }
    },
    "f6b4b9314feb4fb18861d60e037f9edf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
